{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15cfa2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "026bf029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_37312\\812488320.py:33: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df[\"RegistrationDate\"] = pd.to_datetime(\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_37312\\812488320.py:45: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
      "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
      "  cat_cols = df.select_dtypes(include=[\"object\"]).columns\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete! Train/Test sets saved in data/train_test/\n"
     ]
    }
   ],
   "source": [
    "# src/preprocessing.py\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    input_file=\"../data/raw/retail_customers_COMPLETE_CATEGORICAL.csv\",\n",
    "    output_dir=\"data/train_test/\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Full preprocessing pipeline for retail customer dataset.\n",
    "\n",
    "    Inputs:\n",
    "        input_file: str, path to raw CSV file\n",
    "        output_dir: str, path to save processed datasets and transformers\n",
    "    Outputs:\n",
    "        X_train, X_test, y_train, y_test: processed datasets\n",
    "    \"\"\"\n",
    "\n",
    "    # -------------------------\n",
    "    # 1. Load data\n",
    "    # -------------------------\n",
    "    df = pd.read_csv(input_file, encoding='utf-8')\n",
    "\n",
    "    # -------------------------\n",
    "    # 2. Feature Engineering\n",
    "    # -------------------------\n",
    "    df[\"MonetaryPerDay\"] = df[\"MonetaryTotal\"] / (df[\"Recency\"] + 1)\n",
    "    df[\"AvgBasketValue\"] = df[\"MonetaryTotal\"] / (df[\"Frequency\"] + 1)\n",
    "    df[\"TenureRatio\"] = df[\"Recency\"] / (df[\"CustomerTenureDays\"] + 1)\n",
    "\n",
    "    # -------------------------\n",
    "    # 3. Parsing Dates\n",
    "    # -------------------------\n",
    "    df[\"RegistrationDate\"] = pd.to_datetime(\n",
    "        df[\"RegistrationDate\"], dayfirst=True, errors=\"coerce\"\n",
    "    )\n",
    "    df[\"RegYear\"] = df[\"RegistrationDate\"].dt.year\n",
    "    df[\"RegMonth\"] = df[\"RegistrationDate\"].dt.month\n",
    "    df[\"RegDay\"] = df[\"RegistrationDate\"].dt.day\n",
    "    df[\"RegWeekday\"] = df[\"RegistrationDate\"].dt.weekday\n",
    "\n",
    "    # -------------------------\n",
    "    # 4. Handle missing values\n",
    "    # -------------------------\n",
    "    num_cols = df.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    cat_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "\n",
    "    num_imputer = SimpleImputer(strategy=\"median\")\n",
    "    cat_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "    df[num_cols] = num_imputer.fit_transform(df[num_cols])\n",
    "    df[cat_cols] = cat_imputer.fit_transform(df[cat_cols])\n",
    "\n",
    "    # -------------------------\n",
    "    # 5. Encode categorical variables\n",
    "    # -------------------------\n",
    "    ordinal_map = {\n",
    "        \"AgeCategory\": {\"18-24\":1, \"25-34\":2, \"35-44\":3, \"45-54\":4, \"55-64\":5, \"65+\":6, \"Inconnu\":0},\n",
    "        \"SpendingCategory\": {\"Low\":1, \"Medium\":2, \"High\":3, \"VIP\":4},\n",
    "        \"BasketSizeCategory\": {\"Petit\":1, \"Moyen\":2, \"Grand\":3, \"Inconnu\":0},\n",
    "        \"LoyaltyLevel\": {\"Nouveau\":1, \"Jeune\":2, \"Etabli\":3, \"Ancien\":4, \"Inconnu\":0},\n",
    "        \"ChurnRiskCategory\": {\"Faible\":1, \"Moyen\":2, \"Élevé\":3, \"Critique\":4}\n",
    "    }\n",
    "\n",
    "    for col, mapping in ordinal_map.items():\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].map(mapping)\n",
    "\n",
    "    one_hot_cols = [\n",
    "        \"RFMSegment\", \"CustomerType\", \"FavoriteSeason\", \"PreferredTimeOfDay\", \n",
    "        \"Region\", \"WeekendPreference\", \"ProductDiversity\", \"Gender\", \"AccountStatus\"\n",
    "    ]\n",
    "    df = pd.get_dummies(df, columns=[c for c in one_hot_cols if c in df.columns], drop_first=True)\n",
    "\n",
    "    # -------------------------\n",
    "    # 6. Split features and target\n",
    "    # -------------------------\n",
    "    target_col = \"Churn\"\n",
    "    X = df.drop(columns=[target_col, \"NewsletterSubscribed\", \"LastLoginIP\", \"RegistrationDate\"])\n",
    "    y = df[target_col]\n",
    "\n",
    "    # -------------------------\n",
    "    # 7. Train/Test Split\n",
    "    # -------------------------\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    # -------------------------\n",
    "    # 8. Feature Scaling (prevent data leakage)\n",
    "    # -------------------------\n",
    "    num_cols = X_train.select_dtypes(include=[\"int64\", \"float64\"]).columns\n",
    "    scaler = StandardScaler()\n",
    "    X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    # -------------------------\n",
    "    # 9. Save processed datasets and transformers\n",
    "    # -------------------------\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    X_train.to_csv(os.path.join(output_dir, \"X_train.csv\"), index=False)\n",
    "    X_test.to_csv(os.path.join(output_dir, \"X_test.csv\"), index=False)\n",
    "    y_train.to_csv(os.path.join(output_dir, \"y_train.csv\"), index=False)\n",
    "    y_test.to_csv(os.path.join(output_dir, \"y_test.csv\"), index=False)\n",
    "\n",
    "    joblib.dump(scaler, os.path.join(output_dir, \"scaler.joblib\"))\n",
    "    joblib.dump(num_imputer, os.path.join(output_dir, \"num_imputer.joblib\"))\n",
    "    joblib.dump(cat_imputer, os.path.join(output_dir, \"cat_imputer.joblib\"))\n",
    "\n",
    "    print(f\"Preprocessing complete! Train/Test sets saved in {output_dir}\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess_data()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
